{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv  \n",
    "import re\n",
    "\n",
    "with open('Sentiment140_data/training.1600000.processed.noemoticon.csv', mode='r', encoding = \"ISO-8859-1\") as csv_file:  \n",
    "    csv_reader = csv.DictReader(csv_file, fieldnames=['target', 'id', 'date', 'flag', 'user', 'text'])\n",
    "    line = 0\n",
    "    with open('tweets.valid', \"w\", encoding=\"utf-8\") as f1:\n",
    "        with open('tweets.train', \"w\", encoding=\"utf-8\") as f2:\n",
    "            for row in csv_reader:\n",
    "                # Clean the training data\n",
    "                # First we lower case the text\n",
    "                text = row[\"text\"].lower()\n",
    "                # remove links\n",
    "                text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',text)\n",
    "                #Remove usernames\n",
    "                text = re.sub('@[^\\s]+','', text)\n",
    "                # replace hashtags by just words\n",
    "                text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "                #correct all multiple white spaces to a single white space\n",
    "                text = re.sub('[\\s]+', ' ', text)\n",
    "                # Additional clean up : removing words less than 3 chars, and remove space at the beginning and the end\n",
    "                text = re.sub(r'\\W*\\b\\w{1,3}\\b', '', text)\n",
    "\n",
    "                text = re.sub('\\x80', '', text)\n",
    "\n",
    "                text = text.strip()\n",
    "                line = line + 1\n",
    "                # Split data into train and validation\n",
    "                if line%20 == 0:\n",
    "                    f1.write(f'__label__{row[\"target\"]} {text}')\n",
    "                    f1.write('\\n')\n",
    "                else:\n",
    "                    f2.write(f'__label__{row[\"target\"]} {text}')\n",
    "                    f2.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:273: UserWarning: \"b' i just received my G8 viola exam.. and its... well... .. disappointing.. :\\\\..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:273: UserWarning: \"b'E3 ON PLAYSTATION HOME IN ABOUT AN HOUR!!!!!!!!!! \\\\../  \\\\../'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "\n",
    "pat1 = r'@[A-Za-z0-9_]+'\n",
    "pat2 = r'https?://[^ ]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "www_pat = r'www.[^ ]+'\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "def tweet_cleaner_updated(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    try:\n",
    "        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        bom_removed = souped\n",
    "    stripped = re.sub(combined_pat, '', bom_removed)\n",
    "    stripped = re.sub(www_pat, '', stripped)\n",
    "    lower_case = stripped.lower()\n",
    "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    words = [x for x  in tok.tokenize(letters_only) if len(x) > 1]\n",
    "    return (\" \".join(words)).strip()\n",
    "\n",
    "cols = ['sentiment','id','date','query_string','user','text']\n",
    "df = pd.read_csv(\"Sentiment140_data/training.1600000.processed.noemoticon.csv\", header=None, \\\n",
    "                 encoding = \"ISO-8859-1\", names=cols)\n",
    "df.drop(['id','date','query_string','user'],axis=1,inplace=True)\n",
    "clean_tweet_texts = []\n",
    "for i in range(len(df['text'])):\n",
    "    clean_tweet_texts.append(tweet_cleaner_updated(df['text'][i]))\n",
    "clean_df = pd.DataFrame(clean_tweet_texts,columns=['text'])\n",
    "clean_df['target'] = df.sentiment\n",
    "clean_df.to_csv('clean_tweet.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "with open('clean_data.json', \"w\", encoding=\"utf-8\") as f1:\n",
    "    for line in open('data.json', 'r'):\n",
    "        dic = json.loads(line)\n",
    "        new_dic = {}\n",
    "        new_dic['repo_name'] = dic['repo_name']\n",
    "        new_dic[\"created_at\"] = dic[\"created_at\"]\n",
    "        new_dic[\"closed_at\"] = dic[\"closed_at\"]\n",
    "        text = dic[\"body\"]\n",
    "        if text:\n",
    "            text = text.lower()\n",
    "\n",
    "            text = text.encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "            text = re.sub('<[^>]+>','', text)\n",
    "            text = re.sub('[\\n]+', ' ', text)\n",
    "            text = re.sub('[\\r]+', ' ', text)\n",
    "            text = re.sub(r'````[^````]+````','', text)\n",
    "            text = re.sub(r'```[^```]+```','', text)\n",
    "            text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',text)\n",
    "            text = re.sub('@[^\\s]+','', text)\n",
    "            text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "            text = re.sub('[\\s]+', ' ', text)\n",
    "            text = re.sub(r'\\W*\\b\\w{1,3}\\b', '', text)\n",
    "            text = re.sub('\\x80', '', text)\n",
    "            text = text.strip()\n",
    "            \n",
    "            new_dic[\"body\"] = text\n",
    "            json.dump(new_dic, f1)\n",
    "            f1.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
