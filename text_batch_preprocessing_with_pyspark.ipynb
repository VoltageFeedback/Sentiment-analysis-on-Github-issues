{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "import requests\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"myAppName\")\n",
    "spark = pyspark.sql.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF_raw = spark.read.json(\"data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1076483"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDF_raw.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF = dataDF_raw.select(dataDF_raw.repo_name, dataDF_raw.body,\\\n",
    "                           dataDF_raw.created_at.cast('timestamp'), dataDF_raw.closed_at.cast('timestamp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF_exa = spark.read.json(\"C:\\\\Users\\\\79241\\\\Desktop\\\\MIE1512\\\\Project\\\\Data\\\\akka_akka\\\\akka_akka_issues_1.json\")\n",
    "data_rows = dataDF_exa.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://github.com/akka/akka/pull/26633 added actorRefWithAck with 1-1 acks and no buffering (single element buffer).\r\n",
      "\r\n",
      "We could also have a variant with a buffer and OverflowStrategy similar to ActorRefSource and SourceQueue.\r\n",
      "\r\n",
      "Then we could also support batch sending into the stream by having having a function that evaluates if an ack should be sent back for a given element and thereby 1-1 acks are not needed. Could be `ackMessage: Any => Option[Any]`, where the first Any is the incoming message and the second is optionally the ack message.\r\n",
      "\r\n",
      "Then one can define a `Source.actorRefWithAck` with buffer size 100 and use it like:\r\n",
      "\r\n",
      "```\r\n",
      "ackMessage = n => if (n % 50 == 0) Some(n) else None\r\n",
      "\r\n",
      "ref ! 1\r\n",
      "ref ! 2\r\n",
      "ref ! 3\r\n",
      "...\r\n",
      "ref ! 99\r\n",
      "// wait for ack 50 if not already received\r\n",
      "ref ! 100\r\n",
      "ref ! 101\r\n",
      "...\r\n",
      "ref ! 149\r\n",
      "// wait for ack 100 if not already received\r\n",
      "...\r\n",
      "```\r\n",
      "\r\n",
      "Inspiration for jmh benchmark can be found in https://github.com/akka/akka/blob/master/akka-bench-jmh/src/main/scala/akka/remote/artery/SendQueueBenchmark.scala#L79\n"
     ]
    }
   ],
   "source": [
    "print(data_rows[0].body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col, lower, regexp_replace\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_1 = dataDF.select('repo_name', 'created_at', 'closed_at', \\\n",
    "                           (lower(regexp_replace('body', \"```(.*?)```\", \"\")).alias('body')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_1 = df_clean_1.select('repo_name', 'created_at', 'closed_at', \\\n",
    "                           (regexp_replace('body', \"@(.*?)\", \"\").alias('body')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_1 = df_clean_1.select('repo_name', 'created_at', 'closed_at', \\\n",
    "                           (regexp_replace('body', \"#(.*?)\", \"\").alias('body')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_1 = df_clean_1.select('repo_name', 'created_at', 'closed_at', \\\n",
    "                           (regexp_replace('body', \"www(.*?)\", \"\").alias('body')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_1 = df_clean_1.select('repo_name', 'created_at', 'closed_at', \\\n",
    "                           (regexp_replace('body', \"http(.*?)\", \"\").alias('body')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_DF_final = df_clean_1.na.drop(subset=[\"body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_DF_final.write.csv(path='\\clean_data.csv', mode=\"append\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('clean_tweet.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
